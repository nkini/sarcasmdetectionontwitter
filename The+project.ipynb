{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import operator\n",
    "import os\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First I reproduce Riloff et al.'s bootstrapping experiments exactly.\n",
    "# The gist of it is: \n",
    "#    - Sarcasm in a sentence can be modeled as \n",
    "#         the presence of a positive sentiment followed by a negative situation.\n",
    "#    - Self-labeled sarcastic tweets, marked #sarcasm or #sarcastic \n",
    "#         were used in a bootstrapping process\n",
    "#         to automatically discover positive and negative phrases.\n",
    "# I start with \n",
    "# a) reproducing this bootstrappping method exactly\n",
    "# b) using other markers of sarcasm such as #yeahright, #not \n",
    "#        to discover the positive and negative phrases\n",
    "# Possible next step:\n",
    "#     Is the bootstrapping method invariant to initialization?\n",
    "#         It should be if it is robust. Try with a different initial word than love."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# \"positive sentiments that are expressed as a \n",
    "#    verb phrase or as a predicative expression \n",
    "#    (predicate adjective or predicate nominal),\n",
    "#    and negative activities or states that can be a\n",
    "#    complement to a verb phrase.\"\n",
    "\n",
    "# \"we try to recognize these syntactic structures heuristically using\n",
    "#     only part-of-speech tags and proximity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bootstrap(seedword,tweets):\n",
    "    '''\n",
    "    Input: A seed positive word and a set of tweets\n",
    "    Output: A set of positive sentiments and negative situations that indicate a possibly sarcastic tweet\n",
    "\n",
    "    negphrases_count1 = 0\n",
    "    posphrases_count1 = 1\n",
    "    \n",
    "    new_neg_phrases = learn_new_negative_phrases(seedword,tweets)\n",
    "    neg_phrases_count2 = len(new_neg_phrases) + neg_phrases_count1\n",
    "    \n",
    "    while neg_phrases_count1 != neg_phrases_count2 and pos_phrases_count1\n",
    "    \n",
    "    new_pos_phrases = []\n",
    "    for neg_phrase in new_neg_phrases:\n",
    "        new_pos_phrases.extend(learn_new_positive_phrases(neg_phrase,tweets))\n",
    "        \n",
    "    for pos_phrase in new_pos_phrases\n",
    "    pprint(neg_best_candidates)\n",
    "    '''    \n",
    "    new_neg = learn_new_phrases(seedword,'neg',tweets)\n",
    "\n",
    "    #while (new keep getting added)\n",
    "\n",
    "    new_pos = []\n",
    "    for p in new_neg:\n",
    "        new_pos.extend(learn_new_phrases(p,'pos',tweets))\n",
    "    \n",
    "    for p in new_pos:\n",
    "        new_neg.extend(learn_new_phrases(p,'pos',tweets))\n",
    "    \n",
    "    \n",
    "def learn_new_phrases(phrases,pos_or_neg,tweets):\n",
    "\n",
    "    print('Looking for {} phrases'.format(pos_or_neg))\n",
    "\n",
    "    candidates_sarc = defaultdict(int)\n",
    "    candidates_no_sarc = defaultdict(int)\n",
    "\n",
    "    for tweet in tweets:\n",
    "        for seed in phrases:\n",
    "            if pos_or_neg == 'pos':\n",
    "                candidate_phrases = get_possibly_pos_phrase(seed,tweet)\n",
    "            elif pos_or_neg == 'neg':\n",
    "                candidate_phrases = get_possibly_neg_phrase(seed,tweet)\n",
    "\n",
    "            for phrase in candidate_phrases:\n",
    "                if pos_or_neg == 'pos':\n",
    "                    res = pos_phrase_with_desired_syntactic_structure(phrase)\n",
    "                elif pos_or_neg == 'pos_pred':\n",
    "                    res = pos_pred_phrase_with_desired_syntactic_structure(phrase)\n",
    "                elif pos_or_neg == 'neg':\n",
    "                    res = neg_phrase_with_desired_syntactic_structure(phrase)\n",
    "\n",
    "                for p in res:\n",
    "                    if tweet['label'] == 'SARCASM':\n",
    "                        candidates_sarc[p] += 1\n",
    "                    elif tweet['label'] == 'NOT_SARCASM':\n",
    "                        candidates_no_sarc[p] += 1\n",
    "    \n",
    "    return best_candidates(candidates_sarc,candidates_no_sarc,pos_or_neg)\n",
    "    \n",
    "    \n",
    "def best_candidates(sarc_phrase_counts,not_sarc_phrase_counts):\n",
    "    prob = {}\n",
    "    for phrase,count in sarc_phrase_counts.items():\n",
    "        if count >= 3:\n",
    "            if phrase not in not_sarc_phrase_counts:\n",
    "                #TODO: Is this the right thing to do\n",
    "                #  or can there be more nuance?\n",
    "                prob[phrase] = 1\n",
    "            else:\n",
    "                prob[phrase] = count/not_sarc_phrase_counts[phrase]\n",
    "        else:\n",
    "            print(\"Discarded cuz too few ({})\".format(count),phrase)\n",
    "    prob = sorted(prob.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    res = [x[0] for x in prob[:20] if x[1]>=0.8]\n",
    "    print(\"Discarded cuz not probable enough:\",prob[len(res):])\n",
    "    return res\n",
    "    \n",
    "        \n",
    "#The 7 POS bigram patterns are:\n",
    "#  V+V, V+ADV, ADV+V, “to”+V,\n",
    "#  V+NOUN, V+PRO, V+ADJ\n",
    "def has_POS_bigram_pattern(tokens,tags):\n",
    "    if tokens[0] == 'to' and tags[1] == 'V':\n",
    "        return True\n",
    "    if tags[0] == 'R' and tags[1] in ['V','T']:\n",
    "        return True\n",
    "    if tags[0] in ['V','T'] and tags[1] in ['N','O','^','S','Z','A','V','R','T']:\n",
    "        return True\n",
    "\n",
    "'''\n",
    "The 20 POS trigram patterns are designed to capture \n",
    "    seven general types of verb phrases: \n",
    "        #verb and adverb mixtures,\n",
    "        #an infinitive VP that includes an adverb, \n",
    "        a verb phrase followed by a noun phrase, \n",
    "        a verb phrase followed by a prepositional phrase,\n",
    "        a verb followed by an adjective phrase, \n",
    "        #or an infinitive VP followed by an adjective, noun, or pronoun.\n",
    "        \n",
    "Made my own thing up with this as a guide:\n",
    "    http://examples.yourdictionary.com/verb-phrase-examples.html\n",
    "    \n",
    "TODO: This certainly needs more attention\n",
    "'''\n",
    "def has_POS_trigram_pattern(tokens,tags):\n",
    "    third_tag_candidates = set(['N','O','^','S','Z','A','V','R','T','P'])\n",
    "    if tokens[0] == 'to' and tags[1] == 'V' and tags[2] in third_tag_candidates:\n",
    "        return True\n",
    "    if tags[0] == 'V' and tags[1] == 'V' and tags[2] in third_tag_candidates:\n",
    "        return True\n",
    "    \n",
    "def pos_pred_phrase_with_desired_syntactic_structure(phrase):\n",
    "    '''\n",
    "    To learn predicative expressions, we use 24 copular\n",
    "    verbs from Wikipedia and their inflections'''\n",
    "    copular_verbs = {'act','acted','acting','acts','appear','appeared','appearing','appears','are','be','became','become','becomes','becoming','bled','bleed','bleeding','bleeds','came','come','comes','coming','constitute','constituted','constitutes','constituting','die','died','dies','dying','end','ended','ending','ends','fall','falling','falls','feel','feeling','feels','fell','felt','freeze','freezes','freezing','froze','get','gets','getting','go','goes','going','got','grew','grow','growing','grows','is','keep','keeping','keeps','kept','look','looked','looking','looks','prove','proved','proves','proving','ran','remain','remained','remaining','remains','run','running','runs','seem','seemed','seeming','seems','shine','shines','shining','shone','smell','smelled','smelling','smells','sound','sounded','sounding','sounds','stay','stayed','staying','stays','taste','tasted','tastes','tasting','turn','turned','turning','turns','was','wax','waxed','waxes','waxing','went','were'}\n",
    "    \n",
    "    '''\n",
    "    We\n",
    "    extract positive sentiment candidates by extracting\n",
    "    1-grams, 2-grams, and 3-grams that appear immediately\n",
    "    after a copular verb and occur within 5 words\n",
    "    of the negative situation phrase, on either side. \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    We then apply POS patterns to identify n-grams\n",
    "    that correspond to predicate adjective and predicate\n",
    "    nominal phrases. For predicate adjectives, we retain\n",
    "    ADJ and ADV+ADJ n-grams. We use a few\n",
    "    heuristics to check that the adjective is not part of a\n",
    "    noun phrase (e.g., we check that the following word\n",
    "    is not a noun). For predicate nominals, we retain\n",
    "    ADV+ADJ+N, DET+ADJ+N and ADJ+N n-grams.\n",
    "    We excluded noun phrases consisting only of nouns\n",
    "    because they rarely seemed to represent a sentiment.\n",
    "    The sentiment in predicate nominals was usually\n",
    "    conveyed by the adjective\n",
    "    '''\n",
    "\n",
    "#TODO: This could use a 4-gram pattern for better accuracy\n",
    "def neg_phrase_with_desired_syntactic_structure(phrase):\n",
    "    # The inverted order should take care of subsumption\n",
    "    \n",
    "    if len(phrase['tokens']) == 3 and has_POS_trigram_pattern(phrase['tokens'],phrase['tags']):\n",
    "        return [tuple(phrase['tokens'][:3])]\n",
    "    \n",
    "    if len(phrase['tokens']) >= 2 and has_POS_bigram_pattern(phrase['tokens'][:2],phrase['tags'][:2]):\n",
    "        return [tuple(phrase['tokens'][:2])]\n",
    "\n",
    "    if len(phrase['tokens']) >= 1 and phrase['tags'][0] == 'V':\n",
    "        return [tuple(phrase['tokens'][:1])]\n",
    "\n",
    "    return [tuple()]\n",
    "    \n",
    "    \n",
    "def get_possibly_neg_phrase(pos_phrase,tweetobj):\n",
    "    '''\n",
    "    In: a positive sentiment word or phrase and a tweet object\n",
    "    Out: 1,2,3-grams of words on the right of the given phrase\n",
    "    '''\n",
    "    out = []\n",
    "    tokens = tweetobj['tokens']\n",
    "    postags = tweetobj['tags']\n",
    "    len(poss)\n",
    "    for i,token in enumerate(tokens):\n",
    "        if token == pos_phrase:\n",
    "            subout = {'tokens':[],'tags':[]}\n",
    "            j = 1\n",
    "            while j + i < len(tokens) and j <= 3:\n",
    "                subout['tokens'].append(tokens[i+j])\n",
    "                subout['tags'].append(postags[i+j])\n",
    "                j += 1\n",
    "            out.append(subout)\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_possibly_pos_phrase(neg_phrase,tweetobj):\n",
    "    '''\n",
    "    In: a positive sentiment word or phrase and a tweet object\n",
    "    Out: 1,2-grams of words on the left of the given phrase\n",
    "    '''\n",
    "    out = []\n",
    "    tokens = tweetobj['tokens']\n",
    "    postags = tweetobj['tags']\n",
    "    for i,token in enumerate(tokens):\n",
    "        if token == pos_phrase:\n",
    "            subout = {'tokens':[],'tags':[]}\n",
    "            j = 1\n",
    "            while j + i < len(tokens) and j <= 3:\n",
    "                subout['tokens'].append(tokens[i+j])\n",
    "                subout['tags'].append(postags[i+j])\n",
    "                j += 1\n",
    "            out.append(subout)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded cuz too few (1) working\n",
      "Discarded cuz too few (2) doing\n",
      "Discarded cuz too few (1) being called\n",
      "Discarded cuz too few (2) being\n",
      "Discarded cuz too few (1) reading\n",
      "Discarded cuz too few (1) getting\n",
      "Discarded cuz too few (1) being talked down\n",
      "Discarded cuz too few (1) getting lied to\n",
      "Discarded cuz too few (2) getting up\n",
      "Discarded cuz too few (1) missing\n",
      "Discarded cuz too few (1) to see\n",
      "Discarded cuz too few (1) waking\n",
      "Discarded cuz too few (1) being up\n",
      "Discarded cuz too few (2) feeling\n",
      "Discarded cuz too few (1) going\n",
      "Discarded cuz too few (1) getting yelled at\n",
      "Discarded cuz too few (1) being able\n",
      "Discarded cuz too few (1) having\n",
      "Discarded cuz too few (1) freaking being ignored\n",
      "Discarded cuz too few (1) to do\n",
      "Discarded cuz too few (1) being left to\n",
      "Discarded cuz too few (1) waiting\n",
      "Discarded cuz too few (1) not sleeping\n",
      "Discarded cuz not probable enough: []\n",
      "['waking up']\n"
     ]
    }
   ],
   "source": [
    "def test_get_possibly_neg_phrase():\n",
    "    for tweet in original_tweets:\n",
    "        if tweet['label'] == 'SARCASM':\n",
    "            res = get_possibly_neg_phrase('love',tweet)\n",
    "            if res:\n",
    "                print(res)\n",
    "\n",
    "#test_get_possibly_neg_phrase()\n",
    "\n",
    "bootstrap('love',original_riloff_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#START: Creating the Riloff dataset and dumping it in a ready to use format\n",
    "\n",
    "original_riloff_tweets = []\n",
    "with open('sarcasm-annos-emnlp13-tweets.tsv') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            tweetid,label,tweet = line.strip().split('\\t')\n",
    "            original_riloff_tweets.append({'id':int(tweetid),'label':label,'tweet':tweet})\n",
    "        except ValueError:\n",
    "            print(line)\n",
    "            \n",
    "original_riloff_tweets.append({'appeasing_dummy':None})\n",
    "            \n",
    "i = 0\n",
    "original_riloff_tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "with open('riloff-tweets.tagged') as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '':\n",
    "            i += 1\n",
    "            original_riloff_tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "        else:\n",
    "            word,tag,conf = line.strip().split()\n",
    "            original_riloff_tweets[i]['tokens'].append(word)\n",
    "            original_riloff_tweets[i]['tags'].append(tag)\n",
    "            original_riloff_tweets[i]['conf'].append(conf)\n",
    "            \n",
    "pickle.dump(original_riloff_tweets, open('data/march22/riloff-tokenized-and-tagged.pkl','wb'))\n",
    "\n",
    "#END: Creating the Riloff dataset and dumping it in a ready to use format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "original_riloff_tweets = pickle.load(open('data/march22/riloff-tokenized-and-tagged.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seenalready = set()\n",
    "with open('data/march22/shereen-sarc.orig','w') as fw:\n",
    "    for filename in glob.iglob('data/shereen/sarcasm-more-week*.tweet'):\n",
    "        #print(filename)\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                if line not in seenalready:\n",
    "                    fw.write(line)\n",
    "                    seenalready.add(line)\n",
    "                    #They're all unique, turns out.\n",
    "                    #And there's 75680 of them.\n",
    "    with open('data/shereen/sarcasm.tweet') as f:\n",
    "        for line in f:\n",
    "            if line not in seenalready:\n",
    "                fw.write(line)\n",
    "                seenalready.add(line)\n",
    "                #There might be one here that is a repeat, considering the entire set\n",
    "                \n",
    "print(len(seenalready))\n",
    "!wc -l data/march22/shereen-sarc.orig\n",
    "\n",
    "seenalready = set()\n",
    "repeat_count = 0\n",
    "with open('data/march22/shereen-not_sarc.orig','w') as fw:\n",
    "    for filename in glob.iglob('data/shereen/random-*not-sarc*.pkl'):\n",
    "        with open(filename,'rb') as f:\n",
    "            for line in pickle.load(f):\n",
    "                if line not in seenalready:\n",
    "                    fw.write(line+'\\n')\n",
    "                    seenalready.add(line)\n",
    "                else:\n",
    "                    repeat_count += 1\n",
    "    for filename in glob.iglob('data/shereen/random*.tweet'):\n",
    "        with open(filename) as f:\n",
    "            for line in f:\n",
    "                if line not in seenalready:\n",
    "                    fw.write(line)\n",
    "                    seenalready.add(line)\n",
    "                else:\n",
    "                    repeat_count += 1\n",
    "\n",
    "print(repeat_count)\n",
    "print(len(seenalready))\n",
    "!wc -l data/march22/shereen-not_sarc.orig\n",
    "# This one has plenty repeats, but nearly 2m non-repeating tweets nontheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "original_shereen_sarc_tweets = []\n",
    "with open('data/march22/shereen-sarc.orig') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            tweet = line.strip()\n",
    "            original_shereen_sarc_tweets.append({'label':'SARCASM','tweet':tweet})\n",
    "        except ValueError:\n",
    "            print(line)\n",
    "            \n",
    "original_shereen_sarc_tweets.append({'appeasing_dummy':None})\n",
    "            \n",
    "i = 0\n",
    "original_shereen_sarc_tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "with open('data/march22/shereen-sarc.tagged') as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '':\n",
    "            i += 1\n",
    "            original_shereen_sarc_tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "        else:\n",
    "            word,tag,conf = line.strip().split()\n",
    "            try:\n",
    "                original_shereen_sarc_tweets[i]['tokens'].append(word)\n",
    "                original_shereen_sarc_tweets[i]['tags'].append(tag)\n",
    "                original_shereen_sarc_tweets[i]['conf'].append(conf)\n",
    "            except KeyError:\n",
    "                print(line)\n",
    "                \n",
    "                \n",
    "original_shereen_not_sarc_tweets = []\n",
    "with open('data/march22/shereen-not_sarc.cleaned.orig') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            tweet = line.strip()\n",
    "            original_shereen_not_sarc_tweets.append({'label':'NOT_SARCASM','tweet':tweet})\n",
    "        except ValueError:\n",
    "            print(line)\n",
    "            \n",
    "original_shereen_not_sarc_tweets.append({'appeasing_dummy':None})\n",
    "            \n",
    "i = 0\n",
    "original_shereen_not_sarc_tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "with open('data/march22/shereen-not_sarc.cleaned.tagged') as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '':\n",
    "            i += 1\n",
    "            original_shereen_not_sarc_tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "        else:\n",
    "            word,tag,conf = line.strip().split()\n",
    "            original_shereen_not_sarc_tweets[i]['tokens'].append(word)\n",
    "            original_shereen_not_sarc_tweets[i]['tags'].append(tag)\n",
    "            original_shereen_not_sarc_tweets[i]['conf'].append(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(original_shereen_not_sarc_tweets+original_shereen_sarc_tweets,open('data/march22/shereen-tokenized-and-tagged.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "original_shereen_tweets = pickle.load(open('data/march22/shereen-tokenized-and-tagged.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1965169"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2044898"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_shereen_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318916"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "79729*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# So while Riloff's dataset had a 20:80 split of sarc:no_sarc, I had started with a 4:96 split, unfortunately.\n",
    "# I randomly sampled from the 1.9m no_sarc tweets to create a smaller no_sarc dataset that would cause\n",
    "#   a roughly 20:80 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 17688\r\n",
      "-rwxrwxrwx  1 kini  staff  4537730 Mar  7  2016 \u001b[31mscraped-with-JUSTKIDDING-until-30Jun2015.pkl\u001b[m\u001b[m*\r\n",
      "-rwxrwxrwx  1 kini  staff  4512165 Mar  7  2016 \u001b[31mscraped-with-NOT-until-28Dec2015.pkl\u001b[m\u001b[m*\r\n",
      "-rwxrwxrwx  1 kini  staff      784 Mar  7  2016 \u001b[31mthescript.py\u001b[m\u001b[m*\r\n"
     ]
    }
   ],
   "source": [
    "ls -lrt data/scraped/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/search-api/22ndMarch2016:\r\n",
      "\u001b[31m#irony.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31m#jk.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31m#justkidding.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31m#lol.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31m#not.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31m#sarcasm.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31m#sarcastic.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31m#sarcastictweet.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31m#yaright.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31m#yeahright.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31m#yearight.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[30m\u001b[43m__pycache__\u001b[m\u001b[m/\r\n",
      "\u001b[31maccesskeys.py\u001b[m\u001b[m*\r\n",
      "\u001b[31msarcasm+yeahright+justkidding-22ndMarch-9861.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31msarcasm+yeahright+justkidding-9266\u001b[m\u001b[m*\r\n",
      "\u001b[31mtwittersearch.log\u001b[m\u001b[m*\r\n",
      "\u001b[31mtwittersearch.py\u001b[m\u001b[m*\r\n",
      "\r\n",
      "data/search-api/5thMarch2016:\r\n",
      "\u001b[31mirony92.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31mjk1796.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31mjustkidding2603.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31mlol494.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31mnot391.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31msarcasm+yeahright+justkidding-10347\u001b[m\u001b[m*\r\n",
      "\u001b[31msarcasm+yeahright+justkidding-5thMarch-10345.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31msarcasm7178.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31msarcastic96.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31msarcastictweet142.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31myaright35.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31myeahright566.pkl\u001b[m\u001b[m*\r\n",
      "\u001b[31myearight47.pkl\u001b[m\u001b[m*\r\n"
     ]
    }
   ],
   "source": [
    "ls data/search-api/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "contents = {}\n",
    "tweets = []\n",
    "alreadyseen = set()\n",
    "with open('data/march22/scraped-sarc.orig','w') as fw:\n",
    "    for filename in glob.iglob('data/search-api/22ndMarch2016/*.pkl'):\n",
    "        contents[filename] = pickle.load(open(filename,'rb'))\n",
    "        if type(contents[filename][0]) == tuple:\n",
    "            for tup in contents[filename]:\n",
    "                tw_id, tweet = tup\n",
    "                if tw_id not in alreadyseen:\n",
    "                    alreadyseen.add(tw_id)\n",
    "                    tweets.append({'label':'SARCASM','tweet':tweet})\n",
    "                    fw.write('{}\\t{}\\n'.format(tw_id,tweet.replace('\\n',' ')))\n",
    "        elif type(contents[filename][0]) == dict:\n",
    "            for d in contents[filename]:\n",
    "                if d['id'] not in alreadyseen:\n",
    "                    alreadyseen.add(d['id'])\n",
    "                    temp = {'id':d['id'],'label':'SARCASM','tweet':d['text']}\n",
    "                    fn = os.path.basename(filename)\n",
    "                    if fn[0] == '#':\n",
    "                        temp['#type'] = fn[:-4]\n",
    "                    tweets.append(temp)\n",
    "                    fw.write('{}\\t{}\\n'.format(d['id'], d['text'].replace('\\n',' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irony\n",
      "#yaright\n",
      "justkidding\n",
      "not\n",
      "#yeahright\n",
      "#yearight\n",
      "#jk\n",
      "jk\n",
      "#lol\n",
      "sarcasm+yeahright+justkidding-5thMarch-\n",
      "sarcastictweet\n",
      "lol\n",
      "yaright\n",
      "#sarcastic\n",
      "#sarcasm\n",
      "yearight\n",
      "#irony\n",
      "sarcasm\n",
      "#not\n",
      "#sarcastictweet\n",
      "sarcasm+yeahright+justkidding-22ndMarch-\n",
      "#justkidding\n",
      "yeahright\n",
      "sarcastic\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key in contents.keys():\n",
    "    print(remove_trailing_nums(os.path.basename(key)[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25140\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fhasbfhjabdj\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_trailing_nums(text):\n",
    "    return re.sub(\"\\d+$\", \"\", text)\n",
    "\n",
    "print(remove_trailing_nums('fhasbfhjabdj126357'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irony\n",
      "jk\n",
      "justkidding\n",
      "lol\n",
      "not\n",
      "sarcasm+yeahright+justkidding-5thMarch-\n",
      "sarcasm\n",
      "sarcastic\n",
      "sarcastictweet\n",
      "yaright\n",
      "yeahright\n",
      "yearight\n"
     ]
    }
   ],
   "source": [
    "for filename in glob.iglob('data/search-api/5thMarch2016/*.pkl'):\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('data/march22/scraped-sarc.orig','a') as fw:\n",
    "    for filename in glob.iglob('data/search-api/5thMarch2016/*.pkl'):\n",
    "        contents[filename] = pickle.load(open(filename,'rb'))\n",
    "        if type(contents[filename][0]) == tuple:\n",
    "            for tup in contents[filename]:\n",
    "                tw_id, tweet = tup\n",
    "                if tw_id not in alreadyseen:\n",
    "                    alreadyseen.add(tw_id)\n",
    "                    tweets.append({'label':'SARCASM','tweet':tweet})\n",
    "                    fw.write('{}\\t{}\\n'.format(tw_id,tweet.replace('\\n',' ')))\n",
    "        elif type(contents[filename][0]) == dict:\n",
    "            for d in contents[filename]:\n",
    "                if d['id'] not in alreadyseen:\n",
    "                    alreadyseen.add(d['id'])\n",
    "                    temp = {'id':d['id'],'label':'SARCASM','tweet':d['text']}\n",
    "                    fn = os.path.basename(filename)\n",
    "                    if 'sarcasm+yeahright+justkidding-5thMarch-' not in fn:\n",
    "                        temp['#type'] = '#'+remove_trailing_nums(os.path.basename(filename)[:-4])\n",
    "                    tweets.append(temp)\n",
    "                    fw.write('{}\\t{}\\n'.format(d['id'], d['text'].replace('\\n',' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38571\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for filename in glob.iglob('data/scraped/*.pkl'):\n",
    "    contents[filename] = pickle.load(open(filename,'rb'))\n",
    "    #print(type(contents[filename]))\n",
    "    print(type(contents[filename][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('data/march22/scraped-sarc.orig','a') as fw:\n",
    "    for filename in glob.iglob('data/scraped/*.pkl'):\n",
    "        contents[filename] = pickle.load(open(filename,'rb'))\n",
    "        for d in contents[filename]:\n",
    "            if d['id'] not in alreadyseen:\n",
    "                alreadyseen.add(d['id'])\n",
    "                temp = {'id':d['id'],'label':'SARCASM','tweet':d['text']}\n",
    "                fn = os.path.basename(filename)\n",
    "                if fn == 'scraped-with-JUSTKIDDING-until-30Jun2015.pkl':\n",
    "                    temp['#type'] = '#justkidding'\n",
    "                elif fn == 'scraped-with-NOT-until-28Dec2015.pkl':\n",
    "                    temp['#type'] = '#not'\n",
    "                tweets.append(temp)\n",
    "                if type(d['text']) == bytes:\n",
    "                    d['text'] = str(d['text'],'utf-8')\n",
    "                fw.write('{}\\t{}\\n'.format(d['id'], d['text'].replace('\\n',' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-01027454dc5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tags'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'conf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tweets.append({'appeasing_dummy':None})\n",
    "            \n",
    "i = 0\n",
    "tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "with open('data/march22/scraped-sarc.tagged') as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '':\n",
    "            i += 1\n",
    "            tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "        else:\n",
    "            word,tag,conf = line.strip().split()\n",
    "            try:\n",
    "                tweets[i]['tokens'].append(word)\n",
    "                tweets[i]['tags'].append(tag)\n",
    "                tweets[i]['conf'].append(conf)\n",
    "            except KeyError:\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "with open('data/march22/scraped-sarc.orig') as f:\n",
    "    for line in f:\n",
    "        tw_id,tweet = line.strip().split('\\t')\n",
    "        tweets.append({'id':tw_id, 'label':'SARCASM', 'tweet':tweet})\n",
    "            \n",
    "tweets.append({'appeasing_dummy':None})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83568"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "with open('data/march22/scraped-sarc.tagged') as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '':\n",
    "            i += 1\n",
    "            tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "        else:\n",
    "            word,tag,conf = line.strip().split()\n",
    "            tweets[i]['tokens'].append(word)\n",
    "            tweets[i]['tags'].append(tag)\n",
    "            tweets[i]['conf'].append(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'appeasing_dummy': None, 'conf': [], 'tags': [], 'tokens': []}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conf': ['0.9672',\n",
       "  '0.9602',\n",
       "  '0.9914',\n",
       "  '0.9939',\n",
       "  '0.9991',\n",
       "  '0.9978',\n",
       "  '0.9903',\n",
       "  '0.9767',\n",
       "  '0.9993',\n",
       "  '0.8113',\n",
       "  '0.9961',\n",
       "  '0.9847',\n",
       "  '0.9195',\n",
       "  '0.9970',\n",
       "  '0.8172',\n",
       "  '0.5607',\n",
       "  '0.9740',\n",
       "  '0.9938',\n",
       "  '0.9988',\n",
       "  '0.9788',\n",
       "  '0.9962',\n",
       "  '0.9979',\n",
       "  '0.9525',\n",
       "  '0.9942',\n",
       "  '0.5641',\n",
       "  '0.9967'],\n",
       " 'id': '712194231151009792',\n",
       " 'label': 'SARCASM',\n",
       " 'tags': ['D',\n",
       "  'N',\n",
       "  'P',\n",
       "  'N',\n",
       "  'V',\n",
       "  'P',\n",
       "  'N',\n",
       "  'O',\n",
       "  'V',\n",
       "  'V',\n",
       "  '&',\n",
       "  'V',\n",
       "  'P',\n",
       "  'D',\n",
       "  'N',\n",
       "  'V',\n",
       "  'P',\n",
       "  'A',\n",
       "  'N',\n",
       "  'O',\n",
       "  'R',\n",
       "  'V',\n",
       "  'P',\n",
       "  'D',\n",
       "  'N',\n",
       "  ','],\n",
       " 'tokens': ['The',\n",
       "  '#irony',\n",
       "  'of',\n",
       "  'people',\n",
       "  'praying',\n",
       "  'for',\n",
       "  'victims',\n",
       "  'who',\n",
       "  'are',\n",
       "  'killed',\n",
       "  'and',\n",
       "  'injured',\n",
       "  'in',\n",
       "  'a',\n",
       "  'bomb',\n",
       "  'attack',\n",
       "  'by',\n",
       "  'other',\n",
       "  'people',\n",
       "  'who',\n",
       "  'also',\n",
       "  'believe',\n",
       "  'in',\n",
       "  'a',\n",
       "  'god',\n",
       "  '.'],\n",
       " 'tweet': 'The #irony of people praying for victims who are killed and injured in a bomb attack by other people who also believe in a god.'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(tweets,open('scraped-tokenized-and-tagged.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83567"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "shereen_discards = {'pos': {'possibly be', 'why ru', 'look is', 'comes', 'personally love', 'is only', 'not be', 'considering', 'delete', 'are finally', 'spy', 'should experience', 'feeling', 'was totally', 'texting', 'could be', 'taxing', \"didn't try\", 'will be', 'was only', 'really liked', 'are not', 'are now', '#excited', 'actually be', 'enjoy', 'excited', 'absolutely love', \"can't stop\", 'hold', 'is not', 'assume', 'throughly enjoy', 'subjected', 'are just', 'gotta love', 'are already', 'deserve', 'cycled', 'slapped', '#tired', 'conflicted', 'is', 'still be', 'was just', 'pull', 'always love', 'should stop', 'had', 'stealing are', 'miss', 'said was', 'ask', 'guess', 'really are', 'exercise', 'tired', 'hear', 'plan', 'really enjoying', 'are still', \"don't bother\", 'cheating', 'were totally', 'makes', 'is clearly', 'are always', 'just stop', 'admitted', 'deserves', 'is willingly', \"can't resist\", 'appreciated', 'were obviously', 'accused', 'was', 'have been', 'used', 'are', 'make', 'am currently', 'better then', 'doing', 'is #eddiemurphy', 'happens when', 'are screaming', 'not just', 'cancelled', 'loving', 'shpuld be', 'not even', 'so r', 'is simply', \"didn't think\", 'get', 'c', 'hate', 'once again', 'am just', 'think', 'just continue', 'might be', 'also have', 'r nt', 'write', 'leaves', 'started', 'were', 'called', 'love paying', 'knows', 'definitely love', 'apologize', 'end', 'is really', 'help', 'r still', 'go', 'have', 'lose', 'donating', 'be', 'love just', 'see', 'say', \"can't miss\", \"aren't even\", 'looting', 'is just', 'only just', 'is already', 'obviously love', 'is finally', 'more then', 'keep not', 'thinking', 'just love', 'based', 'can keep', 'talking', 'walk', 'love when', 'worry', 'just be', 'teach', 'includes', 'were just', 'keep', 'be hard', \"isn't even\", \"wasn't even\", 'regretting not', 'let', 'should start', 'seeing', 'love', 'r', 'mostly be', 'dont remember', 'do love', 'should be', 'missed', 'can avoid', 'was probably', 'are totally', 'really like'}, 'neg': {'getting invited on', 'getting told who', 'editing', 'getting kicked out', 'to join in', 'just getting', 'drinking coffee', '#mondaymorning', 'being me', 'to write', 'not qualifying', 'taking french', 'watching snapchat', 'to join', 'working retail', 'cancelled flights', 'missing pivotal', 'listening too', 'being stuck tagged', 'work', 'having loads', 'golfing', 'standing awkwardly', 'seeing tyler', 'finding multiple', 'helping people', 'walking', 'having surprise', 'being scheduled to', 'consistently getting', 'to drive ppl', 'trudging', 'closing', 'getting sexually', 'running errands', 'having allergies', 'to give', 'telling', 'delivering leaflets', 'being friends', 'getting played with', 'not talking', 'getting stuck with', 'not eating', 'working alone', 'having class', 'getting calls from', 'finally came', 'traveling', 'lagging', 'spending money', 'fighting', 'throwing alcohol', 'visiting mobile', 'being up', 'not having', 'getting booted off', 'to #laugh', 'finally getting', 'seeing commercials', 'watching', 'being asked to', 'serving', 'eating healthy', 'busing', 're-piercing', 'hate mail', 'getting blamed for', 'having random', 'being told when', 'writing', 'being taught about', 'love finding out', 'reading books', 'cuddles', 'love love to', 'waiting hours', 'being so', 'asking someone', 'randomly waking', 'knowing you', 'feeling part', 'being ignored by', 'hurt', 'wearing new', 'having such', 'sleeping', 'to help you', 'finding', 'working saturday', 'hearing shit', 'being spoken to', 'learning new', 'downloading nfl', 'how spring', 'queueing', 'being woken up', 'being poisoned', 'coming home', 'doing nothing', 'to win though', 'watching hockey', 'getting texted back', 'using what', 'getting shit', 'to do', 'working them', 'being sent home', 'not been', 'being preached about', 'being cheated on', 'getting sick', 'being told i', 'being called off', 'catching', 'having', 'having construction', 'walking outside', 'seeing it', 'eating starapples', 'to help', 'playing', 'being verbally', 'talking', 'wasting hours', 'seeing couples', 'surprise dentist', 'to change', 'put you', 'going grocery', 'is overwhelming', 'to be sarcastic', 'being able', 'being called ugly', 'feeling important', 'love looking at', 'doing that', 'getting mail', 'developing new', 'being schizo', 'getting spammy', 'being followed by', 'supporting', 'being treated differently', 'being pushed away', 'love', 'being hit over', 'doing so', 'getting patched', 'bring interrupted during', 'washing dishes', 'staying up', 'to stress about', 'seein', 'being injured', 'getting yelled at', 'to give you', 'working already', 'being screwed over', 'to wait', 'to send you', 'understanding', 'going places with', 'being covered in', 'doing group', 'having strep', 'getting identical', 'camping', 'punching myself', 'doing maths', 'having friends', 'seeing pictures', 'can i', 'being part', 'hearing ppl', 'seeing skiles', 'closing deli', 'discovering when', 'being generalized', 'feeling', 'composing giant', 'having headaches', 'running', 'being treated like', 'seeing this', 'updating apps', 'to tweet about', 'watching nba', 'scraping ice', 'cuddling', 'writing witty', 'smelling tobacco', 'being indirectly', 'painting', 'being stood up', 'finding vibrators', 'being abused by', 'paying so', 'outlines', 'being catcalled', 'lovee having', 'being randomly', 'texting absolutely', 'how secure', 'having insiders', 'telling people', 'smelling diesel', 'spending', 'being guilt', 'getting motion', 'getting errors', 'being blamed for', 'getting snap chats', 'arriving', 'scoring goals', 'finding shit', 'making plans', 'standing up', 'to see kt', 'hauling laundry', 'being excited for', 'sticking', 'to read this', 'getting called in', 'being depressed', 'wants', 'being called out', 'watching highlights', 'doing paperwork', 'to stay in', 'inhaling smoke', 'being stuck behind', 'sleeping alone', 'being excluded', 'to fail at', 'seeing snow', 'getting frostbite', 'being confused', 'getting', 'doing what', 'making them', 'seeing how', 'being trolled by', 'working extra', 'losing', 'being ignored so', 'playing phone', 'love this', 'hearing rush', 'looking', 'how organised', 'being british', 'falling', 'sitting alone', 'working when', 'celebrating', 'to hear about', 'being super', 'being invisible', 'to throw', 'being lied to', 'being disrespected in', 'is blind', 'doing', 'studying', 'being patronised', 'seeing knee', 'been cold', 'getting really', 'smelling cigarette', 'being questioned by', 'being stopped in', 'reading recipe', 'just watching', 'sitting home', 'constantly being', 'being called at', 'spending spring', 'listening', 'being rlly', 'to see great', 'being stuck at', 'to be around', 'showing up', 'hiding', 'receiving emails', 'to take', 'being surrounded by', 'feeling anxious', 'hearing you', 'being lied too', 'love lovee having', 'writing essays', 'expressing how', 'watching ps4/xboxone', 'seeing mexican', 'detoxing', 'not winning', \"don't like\", 'taking work', 'taking cold', 'to be #sarcastic', 'doing hw', 'quarrelling', 'to be treated', 'having scoliosis', 'to have', 'leaving work', 'delayed flights', 'being subtweeted', 'being sworn at', 'watching jerry', 'getting slapped on', 'feeling neglected', 'breathing', 'to see brady', 'planning', 'looking manly', 'leaving', 'writing haikus', 'to sit', 'flying', 'eating alone', 'bein hit wit', 'autocorrects', 'to spend', 'seeing selfies', 'to try', 'hearing other', 'getting pictures', 'to punch in', 'love waking up', 'being looked at', 'watching archer', 'running reports', 'to see season', 'to clean out', 'being awoken by', 'going here', 'being copied on', 'continues', 'not feeling', 'to make', 'watching millionaire', 'ruining', \"isn't alchemy\", 'being told how', 'sitting down', 'being left by', 'getting roped into', 'are better', 'being called', 'making up', 'putting', 'wasted power', 'cramming', 'seeing', 'will fuck you', 'doing homework', 'taking showers', 'learning', 'crying', 'shoveling snow', 'eating hot', 'let downs', 'having coughing attacks', 'just being', 'being sick', 'to eat you', 'working so', 'growing up', 'feeling ditched', 'eatting root', 'being exhausted', 'to be made', 'doing projects', 'sittin', '#stoned me', 'gettin up', 'seeing tax', 'watching everyone', 'sitting around', 'storming', 'correcting blue', 'moving', 'to do it', 'being forced to', 'getting music', 'love love this', 'being made out', 'getting stressed out', 'getting unbiased', 'surprise open to', 'how being', 'hearing people', 'being ignored thanks', 'sprinting', 'wailing children', 'live tweeting', 'looking after', 'dodging it', 'meeting booth', 'love love waking', 'being ditched after', 'to avoid', 'seeing that', 'being injury', 'just wasting', 'seeing spoilers', 'diy me', 'is not', 'living', 'getting subbed', 'learn business', 'driving', 'spending sunday', 'finding gray', 'losing money', 'havin', 'is', 'not getting', 'filling out', 'failing', 'being given', 'taking', 'using note', 'to know who', 'being invited to', 'rubbing new', 'tweeting', 'breaking them', 'making homemade', 'getting pushed to', 'being teased by', 'giving great', 'to hear them', 'only getting', 'being unfollowed', 'being told not', 'to see it', 'spending saturday', 'to say', 'to scroll endlessly', 'stripping frame', 'being hit up', 'ending', 'paying more', 'playing people', 'cleaning', 'riding', 'letting', 'being roasted', 'twisting shit', 'teaching', 'getting dirty', 'playing middle', 'revising', 'finding out', 'knowing', 'reading something', 'how telling', 'getting stabbed in', 'working saturdays', 'knowing i', 'being bored', 'to display that', 'eating', 'being used', 'pmsing so', 'getting pulled over', 'being volunteered to', 'updating treatment', 'being called retarded', 'being freezing', 'having alcoholics', 'having divorced parents', 'how knowing', 'being alone', 'bein', 'to support new', 'getting ready', 'seeing sunlight', 'finding typos', 'having bad', 'having anxiety', 'being treated as', 'going', 'getting bloodwork', 'tweet was', 'doing powerpoint', 'see', 'still being', 'spending hours', 'being isolated', 'getting warm/dying flowers', 'fixing problems', 'to hand over', 'love love it', 'eating cereal', 'wowed', 'stalking', 'helping customers', 'sitting right', 'to see someone', 'to go to', 'can screw you', 'is #sarcastic', 'missing league', \"wasn't perfect\", 'to reuse', 'getting cat', 'arguing', 'drowning', 'to hurt others', 'hearing them', 'being outside', 'being called weird', 'being hated so', 'getting blown off', 'living here', 'to get', 'working double', 'grading', 'to be', 'watching cops', 'is playing', 'suffocating', 'stunting', 'to work', 'attending musical', 'is when', 'drinking', 'having nothing', 'being severely', 'workin', 'watching warm', 'working super', 'getting random', 'helping self-absorbed', 'to play with', 'being thrown into', 'to rake', 'watching people', 'being forgotten', 'being yelled', 'being put in', 'being downtown', 'to stay', 'being taken for', 'getting woken up', 'to see in', 'laying awake', 'wearing such', '#sarcasm it', 'having emotional', 'forgetting', 'walking home', 'getting told multiple', 'hearing bass', 'texting', 'watching commercials', 'graves', 'doing work', 'bringing', 'to watch them', 'getting messages', 'being interrupted when', 'being put on', 'driving home', 'not sleeping', 'being here', 'being intl', 'limping', 'having panic', 'having big', 'argueing', 'feeling alone', 'laying', 'finding things', 'to go out', 'being kept awake', 'adding up', 'getting ignored', 'getting passed illegally', 'being patronised because', 'choking', 'being killed by', 'is love', 'rendering', 'traveling when', 'to buy', 'being sad', 'spending precious', 'to hangout with', 'does', 'to meet him', 'being wide', 'getting told', 'having tuition', 'to hate', 'to hear', 'to be threatened', 'paying paying out', 'being discriminated against', 'bugs should be', 'watching scary', 'never being', 'love love', 'to follow everton', 'being such', 'being completely', 'to wake up', 'stayin up', 'cleaning up', 'most- #grading', 'hanging', 'packing', 'reading', 'hanging out', 'playing crappy', 'being screamed at', 'getting anon', 'having drunks', 'getting hit on', 'coming', 'watching movies', 'being stuck in', 'being questioned constantly', 'planning conditions', 'getting phone', 'having vertigo', 'studying inside', 'saying such', 'setting alarms', 'getting called fat', 'watching it', 'is real', 'being grown up', 'to stomp on', 'is soooooooooooo', 'is i', \"fixing people's\", 'being kicked in', 'overhearing', 'asking questions', 'being woken at', 'watching #uk', 'being called diana', 'cruising', 'being yelled at', 'getting snapchats', 'to cheat', 'wasting time', 'wasting money', 'finishing work', 'seeing old', 'sucks', 'is having him', 'being constantly', 'getting cute', 'being awake', 'throwing up', 'could change', 'to sit here', 'doing laundry', 'watching horses', 'sending you', 'socialising alone', 'being left out', 'knows', 'seeing oduya', 'being', 'working sundays', 'getting hour', 'leaving school', 'watching deandre', 'having true', 'changing', 'waking up', 'seeing rodents', 'to see', 'being updated on', 'being stranded without', 'opening', 'seeing people', 'paying off', 'watching unmoving cars', 'blacking out', 'reading secret', 'spending time', 'watching roping in', 'sitting', 'being seriously', 'watching him', 'taking freezing cold', 'being ignored', 'running late', 'tweeting me', 'how exciting', 'when trying', 'watching games', 'waiting', 'to #ragequit when', 'being put second', 'being nocturnal', 'love love looking', 'gettin', 'did', 'seeing apm', 'how open', 'love it', 'getting work', 'having bots', 'coming down', 'doing someone', 'getting spammed on', 'using up', 'being home', 'eating dinner', 'siting', 'quitting', 'working holidays', 'not knowing', 'to see how', 'to sleep yet', 'being lectured about', 'being short', 'actually lifted', 'working', 'love love finding', 'waking'}}\n",
    "riloff_discards = {'pos': set(), 'neg': {'being up', 'being wide', 'working', 'to see', 'getting home', 'waiting', 'going', 'getting', 'waking up', 'freaking being ignored', 'getting lied to', 'being left alone', 'having', 'missing', 'feeling', 'walking', 'being called', 'getting yelled at', 'being left to', 'not sleeping', 'waking', 'reading', 'getting up', 'coming back', 'being talked down', 'doing', 'being called into', 'being able', 'to do', 'being'}}\n",
    "scraped_discards = {'neg': {'sitting', 'love love when', 'calling', 'being peed on', 'off silently', 'off again', 'trekking up', 'weirdly dressed', 'to pretend', 'to take', 'fighting isil', 'procrastinating', 'being bombarded with', 'working opposite', 'catching up', 'being low-key', 'feeling embarrassed about', 'paying taxes', 'closing', 'called', 'blended families', 'hearing classical', 'leafleting', 'getting thrown up', 'being wide', 'sound', 'did #not', 'repeating myself', 'wearing wet', 'love love that', 'following', 'being woken up', 'going out', 'smelling', 'to see a-rod', 'to clean', 'giving', 'wasting half', 'watching this', 'helping them', 'visiting', 'ruled', 'to get them', 'to bed early', 'seeing denver', 'being sicker', 'to be ripped', 'singing', 'breaking out', 'updating', 'erecting scaffolding', \"working sunday's\", 'off attacking', 'getting cancelled on', 'to go to', 'being treated like', 'off when', 'talking', 'seeing porn', 'being put last', 'start talking about', 'standing', \"spending valentine's\", 'being told', 'randomly waking', 'feeling sick', 'looking fab', 'out randomly', 'getting back', 'tick box', 'running', 'getting drinks', 'hearing how', 'has', 'going', 'committed', 'throwing up', 'feeling safe', 'being left out', 'spending weekends', 'seeing', 'love when', 'to #justkidding', 'accommodating them', 'doing so', 'to have', 'almost passing', 'concerned here', 'when flares', 'getting along', 'forgetting', 'losing', 'being home', 'getting made fun', 'getting shit', 'is rare', 'to say', 'upholding', 'crying', 'finally being', 'running so', 'not working', 'being me', 'being terribly', 'defending', 'watching michael', 'subjected', 'making faces', 'watching novice', 'watching fights where', 'adulting', 'memorizing chemical', 'to continue', 'working overtime', 'love me', 'accidentally falling', 'pulling', 'making tuition', 'to tell everyone', 'to get', 'to fill', 'missing jared', 'listening', 'getting air', 'coughing so', 'to be in', 'being pretty', 'being ignored whilst', 'making', 'getting decked in', 'waiting hand', 'going back', 'building html', 'seeing people', 'working nights', 'over entirely', 'may work', 'encouraging #pirates', 'falling asleep', 'to spell literature', 'being blanked', 'tuning in', 'being told how', 'being surrounded by', 'pushing', 'being attacked again', 'doing amrap', 'to see us', 'doing nice', 'filling out', 'happening', 'paying attention', 'seeing fire', 'making themselves', 'is always', 'blacking out', 'being put on', 'how inviting', 'being lied to', 'packing them', 'flying', 'rebooking flights', 'learning new', 'getting home', 'finding tyson', 'being consistently', \"pushing people's\", 'to rewind to', 'never getting', 'getting terrible', 'to set up', 'working outside', 'to shift', 'feeling', 'hating', 'babysitting', 'cleaning up', 'to lie', 'feeling sorry', 'tackling', 'getting lectures', 'love love', 'to walk', 'reading', 'was sent back', 'printing books', 'working night', 'to hear seagulls', 'to make new', 'to turn', 'knuckling', 'fucking up', 'leaving work', 'knows not', 'crying myself', 'having sleep', 'to see him', 'to catch up', 'excited', 'riding', 'spending friday', 'up shortly', 'is combine work', 'to see it', 'to come visit', 'folding laundry', 'affecting', 'to hear how', 'welcome here', 'having loud', 'suffering', 'having bad', 'finding', 'overthinking late', 'never sleeping', 'being emotional', 'working', 'to see you', 'feeling disposable', 'being not', 'entitled', 'leaving long', 'backfires', 'to show what', 'to be ignored', 'to spend', 'being so', 'is letting', 'is supporting donald', 'making this', 'making me', 'to sing bt', 'coming', 'put up', 'missing', 'carrying', 'being single', 'is really', 'to be', 'picking', 'to sell', 'studying wounds', 'hurt', 'being alone', 'staying up', 'marching', 'smelling cow', '#defeated', 'writing lesson', '#willing', 'to make', 'getting lectured about', 'playing', 'being', 'getting me', 'can do wonders', 'writing ssows on', 'being cold', '#trump', 'to give', 'being able', 'studying insects', 'to hear what', 'to be at', 'sitting in', 'doing laundry', 'being canadian', 'being power', 'to hear', 'being emotionally', 'is merely', 'being told to', 'paying off', 'to see', 'handles', 'to make you', 'getting calls at', 'never having', 'to have andy', 'to calm', 'having homework', 'getting nudes', 'to begin', 'to share', 'overhearing conversations', 'being hung over', 'getting', 'waiting', 'to raise', 'can take up', 'deicing', 'to try', 'to c yr', 'cleaning tack', 'chillin', 'to marry', 'having long', 'to tweet sad', 'getting harassed by', 'seen', 'feeling included', 'being ignored', 'to see that', 'feeling pain', 'to watch', 'hitting', 'digging', 'being good', 'getting woken up', \"seeing lauren's\", 'making videos', 'sleeping so', 'working closing shifts', 'having nose', 'being taken for', 'to be revelant', 'breaking down', 'to make indians', 'being voluntold', 'getting played', 'writing papers', 'to applaud', 'coming home', 'hearing bad', 'playing ball', 'getting let down', 'dealing', 'being understaffed', 'ignore karlee', 'driving', 'getting ignored', 'being hit by', 'seeing tweets', 'working weekends', 'not having', 'to save', 'looking', 'jackin', 'to see photos', 'made', 'being taxi', 'used', 'tidying', 'hearing people', 'ending', '#heading', 'to do', 'talks', 'to start ticketing', 'seeing jim', 'eating sand', 'to get nominated', 'makes us', '#woke', 'finishing', 'living', 'not getting', 'paying hard', 'to study hitler', 'being late', 'getting forced into', 'packing', 'to give back', 'to drive', 'handwashing me', 'tossing me', 'getting attacked by', 'to laugh at', 'to listened to', 'decorating', 'being delayed', 'helping', 'to sit', 'learning', 'says', 'when shows', 'being used', 'cancelled classes', 'cutting ties', 'being told that', 'making fun', 'injured', 'seeing enzo', 'growing up', 'messing things', 'up when', 'rocks so', 'assumes he', 'doing them', 'being kept in', 'readjusting you', 'being told by', 'walking outside', 'turning', 'to rip on', 'getting cute', 'paying', 'seeing white', 'to invest', 'to donate', 'twitch streams', 'feeling loved', 'not hearing', 'to get off', 'watching', 'wakin up', 'finding something', 'getting called into', 'having', 'beingtold', 'staying late', 'to do in', 'getting poked', '#born #leader', 'seeing things', 'to immerse myself', 'to ride', 'to test it', 'having earthquakes', 'watch', 'having you', 'to stop', 'hurts', 'being uninvolved', 'watching sports', 'drawing', 'to meet in', 'having anxiety', 'opening', 'fracking', 'bothered', 'was', 'to wake up', 'to have you', 'giving up', 'helping ppl', 'saying', 'being left alone', 'spending', 'attracting black', 'waking up', 'getting suprised with', 'being told what', 'breaking up', 'cleaning', 'being disrespected by', 'having roommates', 'trying', 'to pull ross', 'to stay connected', 'love love me', 'following you', \"doesn't come together\", 'to fly', 'havin', 'is literally', 'sleeping', 'how equated', 'love that', 'knowing', 'focusing', 'to run errands', 'is not', 'losing money', 'having sex', 'typing meeting minutes', 'being up', 'starting', 'being pale', 'hanging out', 'to meet her', 'scrolling', 'working out', 'impressed', 'standing outside', 'gone', 'puking', 'is when', 'having organic', 'shaking', 'being reunited with', 'getting migraines', 'seeing is when', 'putting', 'taking cold', 'is', 'worrying', 'is food', 'getting walked in', 'working retail', 'seeing ppl', 'to show how', 'randomly bumping', 'gave', 'to serve'}, 'pos': {'asking', 'splits', 'kill', 'will be', 'absolutely love', 'wtf', 'started', 'imagine', 'am', 'not be', 'might keep', 'got stuck', 'wheter', 'hmu', 'too much', 'can start', 'is slowly', 'literally been', 'study', 'being', 'scan not', 'return', 'forgot', 'still is', 'be expected', 'conflicted', 'almost want', 'was always', 'so #not', 'sweating', \"doesn't love\", 'scolded', 'then try', 'said', 'thought was', 'could be', 'now possibly', 'gets', 'yet not', 'falling', 'just love', 'talk', 'be trying', 'realize', \"aren't even\", 'went', 'thinking', 'means', 'told', 'definitely not', 'was told', 'quit', \"isn't\", 'whining', 'was really', 'considering', 'turning', 'swear', 'sign', 'change', 'was not', 'keep', 'am not', 'smiles', \"wasn't really\", 'ever love', 'progressed', 'knows', 'catch', 'rather be', 'totally still', 'too #not', 'is never', 'just luv', 'need more', 'just fail', 'is probably', 'r', 'stop', \"wasn't there\", 'allows', 'tried', 'cant stop', 'allright #not', 'gotta love', 'fired', 'tweet', 'see', 'am literally', 'end', 'spend', 'just have', 'clearly not', 'not still', \"can't wait\", '#not enjoy', 'obviously just', 'am totes', 'say', 'having', 'was sleep', 'getting stuck', 'set', 'cussing', 'not fully', 'say was', 'resort', 'been there', 'is still', \"here isn't\", 'are', 'even bother', 'played when', 'found', 'had', 'imply', 'do', 'now #not', 'else was', 'is beginning', 'can keep', \"can't continue\", 'regret not', 'is just', 'would love', 'believe', 'were born', 'wear is', '#not be', 'kill #not', 'love not', \"don't start\", 'works', 'been', 'sat', 'hold', 'makes', 'hear', 'should b', 'hope', 'loving', 'need', 'drinking is', \"couldn't resist\", 'know just', 'wants', 'feel', \"don't try\", 'was just', 'atleast am', 'freaking love', 'text', 'tired', 'was', 'guess', 'constantly deflecting', 'thought', 'debating quitting', 'protesting', 'just tried', 'collapsed', 'prosper', 'scamming', 'saying', 'spending', \"won't be\", 'is finally', 'texting stop', 'wondering', 'would like', 'gotta start', 'will quit', \"don't have\", 'was spent', 'use', 'just off', 'is #not', 'conned', 'delaying', 'trying', 'injure', 'have', 'just loved', 'am really', 'make be', 'encouraging when', 'believe is', 'r not', 'back then', 'would stop', 'just going', 'watched', 'has', 'am just', 'never ceases', 'know how', 'probably just', 'wasting', 'flirting', 'are never', 'forcing', 'overwhelmed', 'find', 'is ever', 'using', 'are not', 'is seriously', 'got', 'may be', 'serves', 'has been', 'are also', 'excited', 'gonna start', 'were', 'finally finished', 'running is', 'didn’t need', 'just loving', 'walk', 'are really', 'is not', 'stops', 'am totally', 'not even', 'finding', 'inspired', 'be', 'is supposedly', 'rocking', 'was finally', \"haven't had\", 'made', 'please keep', 'working', 'would be', 'kald #not', 'used', 'fallen', 'is already', 'empowering', 'can’t stand', 'manahel keep', 'is enough', 'reminiscing', 'spinning when', 'just now', 'ending', 'have been', \"won't like\", 'else is', 'lived is', 'not enough', 'really appreciate', 'was still', 'was probably', '#breeds', 'stop trying', 'just casually', 'coming', 'really be', 'should go', 'will keep', 'living', 'are totally', 'sleep', 'mean', 'love', 'was contemplating', 'am beginning', 'is really', 'wonder', 'ended', 'think', 'should be', 'seem need', 'spin', 'is', 'work', 'know', 'is truly', 'wait', 'assume', '#sarcasm keep', 'imagining', 'was #not', 'must be', 'really going', 'gonna keep', 'please', 'not actually', 'am always', 'is totally', 'always love', 'got caught', 'want', 'are just', 'knows how', 'really love', 'particularly enjoy', 'cares', '#not going', \"isn't designed\", 'is going', 'tweet was', 'gonna be', 'will start', 'obviously have', 'buffer when', 'go', 'just voluntarily', 'look'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predicate_expr = set([\"great\", \"so much fun\", \"good\", \"so happy\", \"better\", \"my favorite thing\", \"cool\", \"funny\", \"nice\", \"always fun\", \"fun\", \"awesome\", \"the best feeling\", \"amazing\", \"happy\", \"ready today\", \"ready\", \"dry\", \"juicy\", \"my favorite part\"])\n",
    "pos_expr = set([\"love\", \"missed\", \"loves\", \"enjoy\", \"cant wait\", \"excited\", \"wanted\", \"can't wait\", \"get\", \"appreciate\", \"decided\", \"loving\", \"really like\", \"looooove\", \"just keeps\", \"loveee\", \"randomly stop\", \"cannot wait\", \"just live\", \"please keep\", \"live\", \"stoked\", \"goin\", \"reading\", \"break\", \"just stops\", \"stops\"])\n",
    "neg_expr = set([\"being ignored\", \"being sick\", \"waiting\", \"feeling\", \"waking up early\", \"being woken\", \"fighting\", \"staying\", \"writing\", \"being home\", \"cleaning\", \"not getting\", \"crying\", \"sitting at home\", \"being stuck\", \"starting\", \"being told\", \"being left\", \"getting ignored\", \"being treated\", \"doing homework\", \"learning\", \"getting up early\", \"going to bed\", \"getting sick\", \"riding\", \"being ditched\", \"getting ditched\", \"missing\", \"not sleeping\", \"not talking\", \"trying\", \"falling\", \"walking home\", \"getting yelled\", \"being awake\", \"being talked\", \"taking care\", \"doing nothing\", \"wasting\", \"throwing\", \"getting woken up\", \"to spend\", \"standing\", \"smelling\", \"getting woken\", \"arguing\", \"paying bills\", \"being locked\", \"shoveling\", \"getting called\", \"being at work\", \"having nothing\", \"getting invited\", \"getting blown\", \"dealing\", \"ending\", \"to wake\", \"when doesn't text\", \"getting ready\", \"to learn\", \"picking\", \"walking to class\", \"breaking\", \"being invited\", \"getting home\", \"setting\", \"dropping\", \"not seeing\", \"forgetting\", \"being called fat\", \"getting lied\", \"invited\", \"to sit here\", \"to be ignored\", \"being late\", \"doing laundry\", \"being taken\", \"practicing\", \"babysitting\", \"getting hit\", \"being used\", \"being used\", \"being reminded\", \"when falls\", \"working all day\", \"running late\", \"traveling\", \"peeing\", \"being hit\", \"having practice\", \"not being invited\", \"being bored\", \"stepping\", \"spending my day\", \"leaving\", \"almost getting\", \"being put\", \"passing\", \"being at school\", \"to study\", \"going to class\", \"coughing\", \"sitting in traffic\", \"being yelled\", \"fixing\", \"burning\", \"walking to school\", \"wakin\", \"seeing people\", \"being accused\", \"being up early\", \"scratches\", \"texting someone\", \"being invited places\", \"receiving\", \"being grounded\", \"checking\", \"getting my ass\", \"getting back\", \"getting bitched\", \"getting treated\", \"only getting\", \"reviewing\", \"sitting alone\", \"getting screwed\", \"going there\", \"getting stared\", \"calling\", \"watching scary movies\", \"getting no sleep\", \"taking tests\", \"getting locked\", \"reading tweets\", \"teaching\", \"waking up not\", \"sounding\", \"getting made\", \"sleeping alone\", \"not feeling\", \"being surrounded\", \"editing\", \"being stood up\", \"to randomly ask\", \"getting hacked\", \"getting texts\", \"having insomnia\", \"having homework\", \"blamed\", \"showing\", \"being blamed\", \"getting bad news\", \"getting played\", \"being stood\", \"scrolling\", \"being lied too\", \"being a loner\", \"going weeks\", \"being up late\", \"having class\", \"failing\", \"being cussed\", \"listening to women\", \"when ignores\", \"cutting\", \"bring\", \"burnt\", \"getting hate\", \"coming to school\", \"sitting here\", \"waking up early\", \"being called names\", \"getting replaced\", \"having bruises\", \"closing\", \"coming back\", \"getting punched\", \"getting phone\", \"spending all day\", \"being pushed\", \"spending\", \"not being able\", \"waking\", \"working\", \"sitting\", \"walking\", \"coming home\", \"living\", \"being lied\", \"getting\", \"coming\", \"going\", \"running\", \"to sit\", \"being called\", \"to read\", \"studying\", \"paying\", \"texting\", \"hearing\", \"replying\", \"gettin better\", \"gettin better\", \"gettin\", \"eating\", \"losing\", \"listening\", \"to get up\", \"finding\", \"to clean\", \"being able\", \"seeing\", \"to run\", \"to drive\", \"to go back\", \"looking\", \"taking\", \"putting\", \"driving\", \"to start\", \"posting\", \"to pay\", \"telling me\", \"ruined\", \"being woke\", \"hitting\", \"laying\", \"cuddling\", \"reading\", \"buying\", \"cancelled\", \"sending\", \"to see pictures\", \"to find out\", \"sharing\", \"finishing\", \"sweating\", \"to miss\", \"hurting\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS Riloff\n",
      "pos\n",
      "set()\n",
      "neg\n",
      "{'missing', 'feeling', 'working', 'not sleeping', 'coming back', 'going', 'walking', 'being able', 'being called', 'reading', 'getting', 'waiting', 'getting home', 'waking'}\n",
      "pospred\n",
      "set()\n",
      "\n",
      "DS Shereen\n",
      "pos\n",
      "{'really like', 'enjoy', 'get', 'missed', 'excited', 'loving', 'love'}\n",
      "neg\n",
      "{'running late', 'gettin', 'cleaning', 'forgetting', 'working', 'not sleeping', 'closing', 'looking', 'putting', 'reading', 'running', 'spending', 'sitting', 'doing laundry', 'riding', 'only getting', 'to sit here', 'crying', 'being awake', 'feeling', 'getting ignored', 'walking home', 'taking', 'going', 'falling', 'being able', 'being bored', 'not getting', 'doing homework', 'ending', 'getting sick', 'being home', 'studying', 'waking', 'eating', 'getting ready', 'fighting', 'writing', 'getting phone', 'editing', 'to sit', 'texting', 'listening', 'being used', 'cuddling', 'sleeping alone', 'leaving', 'sitting alone', 'being called', 'getting', 'doing nothing', 'being yelled', 'seeing', 'arguing', 'being lied too', 'being ignored', 'getting woken up', 'driving', 'being stood up', 'teaching', 'being sick', 'seeing people', 'living', 'coming', 'laying', 'coming home', 'not talking', 'having nothing', 'traveling', 'not feeling', 'to spend', 'learning', 'walking', 'failing', 'having class', 'waiting', 'finding', 'losing'}\n",
      "pospred\n",
      "{'really like', 'enjoy', 'get', 'missed', 'excited', 'loving', 'love'}\n",
      "\n",
      "DS Scraped\n",
      "pos\n",
      "{'stops', 'please keep', 'excited', \"can't wait\", 'loving', 'love'}\n",
      "neg\n",
      "{'cleaning', 'forgetting', 'standing', 'working', 'scrolling', 'closing', 'finishing', 'looking', 'to clean', 'putting', 'reading', 'to be ignored', 'running', 'babysitting', 'spending', 'sitting', 'doing laundry', 'riding', 'crying', 'feeling', 'picking', 'getting ignored', 'hitting', 'being told', 'going', 'being able', 'not getting', 'smelling', 'ending', 'being home', 'dealing', 'to sit', 'trying', 'listening', 'being used', 'calling', 'finding', 'getting', 'seeing', 'being ignored', 'driving', 'missing', 'getting back', 'to drive', 'seeing people', 'starting', 'having homework', 'living', 'coming', 'coming home', 'getting played', 'to spend', 'learning', 'getting woken up', 'paying', 'waiting', 'getting home', 'being late', 'losing'}\n",
      "pospred\n",
      "{'stops', 'please keep', 'excited', \"can't wait\", 'loving', 'love'}\n"
     ]
    }
   ],
   "source": [
    "# HOW MANY DISCARDS ARE IN RILOFF'S FINAL LIST?\n",
    "print(\"DS Riloff\")\n",
    "print(\"pos\")\n",
    "print(pos_expr & riloff_discards['pos'])\n",
    "print(\"neg\")\n",
    "print(neg_expr & riloff_discards['neg'])\n",
    "print(\"pospred\")\n",
    "print(pos_expr & riloff_discards['pos'])\n",
    "print()\n",
    "print(\"DS Shereen\")\n",
    "print(\"pos\")\n",
    "print(pos_expr & shereen_discards['pos'])\n",
    "print(\"neg\")\n",
    "print(neg_expr & shereen_discards['neg'])\n",
    "print(\"pospred\")\n",
    "print(pos_expr & shereen_discards['pos'])\n",
    "print()\n",
    "print(\"DS Scraped\")\n",
    "print(\"pos\")\n",
    "print(pos_expr & scraped_discards['pos'])\n",
    "print(\"neg\")\n",
    "print(neg_expr & scraped_discards['neg'])\n",
    "print(\"pospred\")\n",
    "print(pos_expr & scraped_discards['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A\n",
    "out.scraped.full_length\n",
    "out.shereen.full_length\n",
    "out.riloff.full_length\n",
    "\n",
    "B\n",
    "out.riloff.full_length.big_change\n",
    "out.shereen.full_length.big_change\n",
    "out.scraped.full_length.big_change\n",
    "\n",
    "C\n",
    "out.riloff.inv_sub\n",
    "out.shereen.inv_sub\n",
    "out.scraped.inv_sub\n",
    "\n",
    "\n",
    "results['A/B/C']['shereen/scraped']['neg/pos/discards']\n",
    "\n",
    "A\n",
    "  scraped\n",
    "    pos\n",
    "      11\n",
    "    discards\n",
    "      pos\n",
    "        312\n",
    "      neg\n",
    "        495\n",
    "    neg\n",
    "      24\n",
    "  shereen\n",
    "    pos\n",
    "      2\n",
    "    discards\n",
    "      pos\n",
    "        166\n",
    "      neg\n",
    "        731\n",
    "    neg\n",
    "      20\n",
    "C\n",
    "  scraped\n",
    "    pos\n",
    "      17\n",
    "    discards\n",
    "      pos\n",
    "        409\n",
    "      neg\n",
    "        269\n",
    "    neg\n",
    "      18\n",
    "  shereen\n",
    "    pos\n",
    "      19\n",
    "    discards\n",
    "      pos\n",
    "        578\n",
    "      neg\n",
    "        336\n",
    "    neg\n",
    "      35\n",
    "B\n",
    "  scraped\n",
    "    pos\n",
    "      18\n",
    "    discards\n",
    "      pos\n",
    "        405\n",
    "      neg\n",
    "        1508\n",
    "    neg\n",
    "      40\n",
    "  shereen\n",
    "    pos\n",
    "      18\n",
    "    discards\n",
    "      pos\n",
    "        481\n",
    "      neg\n",
    "        1239\n",
    "    neg\n",
    "      46\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Code versions:\n",
    "#This contains an interpretation which finds candidate phrases \n",
    "#  looking at only one opposite polarity phrase at a time\n",
    "#  e.g. Get all negative phrases for 'love'. \n",
    "#       Then all negative phrases for 'really like' etc.\n",
    "# A\n",
    "replicate_riloff.py\n",
    "\n",
    "#This contains an interpretation which finds candidate phrases \n",
    "#  looking at the entire opposite polarity set\n",
    "#  e.g. Get all negative phrases for {'love','really like'} etc. \n",
    "# B\n",
    "replicate_riloff-big_correction.py\n",
    "\n",
    "#This contains an interpretation which correctly understands subsumption\n",
    "#  So far e.g. “waiting forever” was chosen over “waiting” \n",
    "#  e.g. Now, waiting is chosen  \n",
    "# C\n",
    "replicate_riloff-inverted_subsumption.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ashish/Desktop/245/Project/code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results = pickle.load(open('results_summary.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'B', 'A'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 456\r\n",
      "-rw-r--r--  1 ashish ashish   1828 Feb 12  2016 tweet-analysis.py\r\n",
      "-rw-r--r--  1 ashish ashish    252 Mar  6  2016 accesskeys.py\r\n",
      "-rw-r--r--  1 ashish ashish   1945 Mar  6  2016 twittersearch.py\r\n",
      "-rw-r--r--  1 ashish ashish   1574 Mar  6  2016 download_tweets_pythonv3.py\r\n",
      "-rw-r--r--  1 ashish ashish   1716 Mar  6  2016 gettweetsbyid.py\r\n",
      "-rw-r--r--  1 ashish ashish  28941 Mar  6  2016 quickjava-2.0.7-fx.xpi\r\n",
      "-rw-r--r--  1 ashish ashish   2429 Mar  7  2016 gettweetsbyscraping2.py\r\n",
      "-rw-r--r--  1 ashish ashish   2182 Mar  7  2016 gettweetsbyscraping.py\r\n",
      "-rw-r--r--  1 ashish ashish   2788 Mar  7  2016 gettweetsbyscraping-selenium.py\r\n",
      "-rw-r--r--  1 ashish ashish   2708 Mar  7  2016 gettweetsbyscraping-selenium2.py\r\n",
      "-rw-r--r--  1 ashish ashish    784 Mar  7  2016 thescript.py\r\n",
      "-rw-r--r--  1 ashish ashish  77020 Mar 21 19:22 Classification of Tweets as Sarcastic or Not Sarcastic.ipynb\r\n",
      "-rw-r--r--  1 ashish ashish 127663 Mar 23 23:32 results_summary.pkl\r\n",
      "-rw-r--r--  1 ashish ashish    410 Mar 23 23:41 print_result_stats.py\r\n",
      "-rw-r--r--  1 ashish ashish  78413 Mar 24 08:55 The project.ipynb.bkp\r\n",
      "drwxr-xr-x  2 ashish ashish   4096 Mar 24 09:53 __pycache__\r\n",
      "drwxr-xr-x  6 ashish ashish   4096 Mar 24 09:53 ark-tweet-nlp-0.3.2\r\n",
      "drwxr-xr-x 10 ashish ashish   4096 Mar 24 09:53 data\r\n",
      "drwxr-xr-x  2 ashish ashish   4096 Mar 24 09:54 temp\r\n",
      "-rw-r--r--  1 ashish ashish  80481 Mar 24 18:11 The project.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "### EXPERIMENTS ###\n",
    "!ls -lrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "riloff_eval_dataset = pickle.load(open('data/riloff-emnlp/sarcasm-annos-emnlp13-tweet_objs-2124.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Very low entries in this giveaway! Hop over and check it out! http://t.co/OrUSN9ne'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "riloff_eval_dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "with open('data/riloff-emnlp/sarcasm-annos-emnlp13-tweets-noids.tsv') as f:\n",
    "    for line in f:\n",
    "        label,tweet = line.strip().split('\\t')\n",
    "        x.append(tweet)\n",
    "        y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "z = list(zip(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    random.shuffle(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x,y = zip(*z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "c = defaultdict(int)\n",
    "for label in y:\n",
    "    c[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'NOT_SARCASM': 1666, 'SARCASM': 458})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "riloff_eval = pickle.load(open('../../riloff-tokenized-and-tagged-lowercase.pkl','rb'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2124"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(riloff_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-41eab4840d3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "bigrams = ngrams(riloff_eval[0]['tokens'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('very', 'low'), ('low', 'entries'), ('entries', 'in'), ('in', 'this'), ('this', 'giveaway'), ('giveaway', '!'), ('!', 'hop'), ('hop', 'over'), ('over', 'and'), ('and', 'check'), ('check', 'it'), ('it', 'out'), ('out', '!'), ('!', 'http://t.co/orusn9ne')]\n"
     ]
    }
   ],
   "source": [
    "print(list(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "categories = ['NOT_SARCASM','SARCASM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_set_x = [tweet.lower().replace('#sarcasm','').replace('#sarcastic','') for tweet in x[:200]]\n",
    "train_set_y = y[:200]\n",
    "test_set_x =  [tweet.lower().replace('#sarcasm','').replace('#sarcastic','') for tweet in x[200:]]\n",
    "test_set_y = y[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1087)\n",
      "(200, 1087)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_set_x)\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "\n",
    "#tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "#X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "#print(X_train_tf.shape)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_set_y)\n",
    "#To try to predict the outcome on a new document we need to extract the features using almost the same feature extracting chain as before. The difference is that we call transform instead of fit_transform on the transformers, since they have already been fit to the training set:\n",
    "#docs_new = [tweetobj['tweet'] for tweetobj in riloff_eval]\n",
    "X_new_counts = count_vect.transform(test_set_x)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "#for doc, category in zip(docs_new, predicted):\n",
    "#    print('%r => %s' % (doc, twenty_train.target_names[category]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78534303534303529"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.mean(predicted == test_set_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7718295218295218"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42)),\n",
    "])\n",
    "_ = text_clf.fit(train_set_x, train_set_y)\n",
    "predicted = text_clf.predict(test_set_x)\n",
    "np.mean(predicted == test_set_y)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    SARCASM       0.85      0.86      0.85      1511\n",
      "NOT_SARCASM       0.47      0.46      0.47       413\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_set_y, predicted,\n",
    "    target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.766632016632\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    SARCASM       0.84      0.86      0.85      1511\n",
      "NOT_SARCASM       0.45      0.41      0.43       413\n",
      "\n",
      "avg / total       0.76      0.77      0.76      1924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42)),\n",
    "])\n",
    "_ = text_clf.fit(train_set_x, train_set_y)\n",
    "predicted = text_clf.predict(test_set_x)\n",
    "#print(np.mean(predicted == test_set_y))         \n",
    "print(metrics.classification_report(test_set_y, predicted,\n",
    "    target_names=categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Moment of truth:\n",
    "results = pickle.load(open('results_summary.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with hinge loss\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "NOT_SARCASM       0.84      0.86      0.85      1511\n",
      "    SARCASM       0.45      0.41      0.43       413\n",
      "\n",
      "avg / total       0.76      0.77      0.76      1924\n",
      "\n",
      "B\n",
      "DatasetSarc\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "NOT_SARCASM       0.79      1.00      0.88      1511\n",
      "    SARCASM       0.69      0.02      0.04       413\n",
      "\n",
      "avg / total       0.77      0.79      0.70      1924\n",
      "\n",
      "DatasetOther\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "NOT_SARCASM       0.79      1.00      0.88      1511\n",
      "    SARCASM       0.50      0.01      0.01       413\n",
      "\n",
      "avg / total       0.72      0.79      0.69      1924\n",
      "\n",
      "A\n",
      "DatasetSarc\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "NOT_SARCASM       0.79      1.00      0.88      1511\n",
      "    SARCASM       0.75      0.01      0.01       413\n",
      "\n",
      "avg / total       0.78      0.79      0.69      1924\n",
      "\n",
      "DatasetOther\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "NOT_SARCASM       0.79      1.00      0.88      1511\n",
      "    SARCASM       1.00      0.00      0.00       413\n",
      "\n",
      "avg / total       0.83      0.79      0.69      1924\n",
      "\n",
      "C\n",
      "DatasetSarc\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "NOT_SARCASM       0.79      1.00      0.88      1511\n",
      "    SARCASM       0.50      0.01      0.03       413\n",
      "\n",
      "avg / total       0.73      0.79      0.70      1924\n",
      "\n",
      "DatasetOther\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "NOT_SARCASM       0.79      1.00      0.88      1511\n",
      "    SARCASM       0.73      0.02      0.04       413\n",
      "\n",
      "avg / total       0.78      0.79      0.70      1924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, n_iter=5, random_state=42)),\n",
    "])\n",
    "_ = text_clf.fit(train_set_x, train_set_y)\n",
    "predicted = text_clf.predict(test_set_x)\n",
    "#print(np.mean(predicted == test_set_y)) \n",
    "print(\"SVM with hinge loss\")\n",
    "print(metrics.classification_report(test_set_y, predicted,\n",
    "    target_names=categories))\n",
    "\n",
    "for _id,resultset in results.items():\n",
    "    print(_id)\n",
    "    for dataset,wordsets in resultset.items():\n",
    "        predicted = []\n",
    "        if dataset == 'shereen': \n",
    "            print('DatasetSarc')\n",
    "        else: \n",
    "            print('DatasetOther')\n",
    "        #print(dataset)\n",
    "        for tweet in test_set_x:\n",
    "            predicted.append('NOT_SARCASM')\n",
    "            for posphrase in wordsets['pos']:\n",
    "                if posphrase in tweet:\n",
    "                    for negphrase in wordsets['neg']:\n",
    "                        if negphrase in tweet:\n",
    "                            predicted[-1] = \"SARCASM\"\n",
    "        print(metrics.classification_report(test_set_y,predicted,target_names=categories))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "c = defaultdict(int)\n",
    "for label in test_set_y:\n",
    "    c[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'NOT_SARCASM': 1511, 'SARCASM': 413})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27332892124420916"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "413/1511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ashish/Desktop/245/Project/code'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
