{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import operator\n",
    "import os\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.util import ngrams\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "riloff_eval_dataset = pickle.load(open('march22/sarcasm-annos-emnlp13-tweet_objs-2124.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_ids = set([tweet['id'] for tweet in riloff_eval_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "riloff_eval_dataset_alt = pd.read_csv('march22/riloff-tweets.orig',delimiter='\\t',names=['tweet id','label','tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_ids_alt = set(riloff_eval_dataset_alt['tweet id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "riloff_orig_dataset = pd.read_csv('march22/sarcasm-annos-emnlp13.tsv',delimiter='\\t',names=['tweet id','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "876"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_ids_orig = set(riloff_orig_dataset['tweet id'])\n",
    "len(all_ids_orig - all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "riloff_orig_dataset2 = riloff_orig_dataset.set_index('tweet id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "riloff_orig_dataset2 = {}\n",
    "for twid,label in zip(riloff_orig_dataset['tweet id'],riloff_orig_dataset['label']):\n",
    "    riloff_orig_dataset2[twid] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_eval_dataset = []\n",
    "for twobj in riloff_eval_dataset:\n",
    "    d = {'id':twobj['id'],'text':twobj['text'],'label':riloff_orig_dataset2[twobj['id']]}\n",
    "    new_eval_dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for d in new_eval_dataset:\n",
    "    if '\\n' in d['text']:\n",
    "        print(d['text'])\n",
    "        print('###')\n",
    "        count += 1\n",
    "\n",
    "print(\"\\n\\n\\ncount\",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('/home/nikhil/Desktop/245/Project/code/ark-tweet-nlp-0.3.2/riloff-orig.json','w') as fw:\n",
    "    for d in new_eval_dataset:\n",
    "        fw.write(json.dumps(d)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "with open('/home/nikhil/Desktop/245/Project/code/ark-tweet-nlp-0.3.2/riloff-orig.tagger.out.conll') as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '':\n",
    "            i += 1\n",
    "            tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "        else:\n",
    "            word,tag,conf = line.strip().split()\n",
    "            tweets[i]['tokens'].append(word)\n",
    "            tweets[i]['tags'].append(tag)\n",
    "            tweets[i]['conf'].append(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(tweets,open('march27/riloff-tokenized-and-tagged.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_trailing_nums(text):\n",
    "    return re.sub(\"\\d+$\", \"\", text)\n",
    "\n",
    "contents = {}\n",
    "tweets = []\n",
    "alreadyseen = set()\n",
    "\n",
    "for filename in glob.iglob('data/search-api/22ndMarch2016/*.pkl'):\n",
    "    contents[filename] = pickle.load(open(filename,'rb'))\n",
    "    if type(contents[filename][0]) == tuple:\n",
    "        for tup in contents[filename]:\n",
    "            tw_id, tweet = tup\n",
    "            if tw_id not in alreadyseen:\n",
    "                alreadyseen.add(tw_id)\n",
    "                tweets.append({'label':'SARCASM','text':tweet})\n",
    "    elif type(contents[filename][0]) == dict:\n",
    "        for d in contents[filename]:\n",
    "            if d['id'] not in alreadyseen:\n",
    "                alreadyseen.add(d['id'])\n",
    "                temp = {'id':d['id'],'label':'SARCASM','text':d['text']}\n",
    "                fn = os.path.basename(filename)\n",
    "                if fn[0] == '#':\n",
    "                    temp['#type'] = fn[:-4]\n",
    "                tweets.append(temp)\n",
    "                    \n",
    "for filename in glob.iglob('data/search-api/5thMarch2016/*.pkl'):\n",
    "    contents[filename] = pickle.load(open(filename,'rb'))\n",
    "    if type(contents[filename][0]) == tuple:\n",
    "        for tup in contents[filename]:\n",
    "            tw_id, tweet = tup\n",
    "            if tw_id not in alreadyseen:\n",
    "                alreadyseen.add(tw_id)\n",
    "                tweets.append({'label':'SARCASM','text':tweet})\n",
    "    elif type(contents[filename][0]) == dict:\n",
    "        for d in contents[filename]:\n",
    "            if d['id'] not in alreadyseen:\n",
    "                alreadyseen.add(d['id'])\n",
    "                temp = {'id':d['id'],'label':'SARCASM','text':d['text']}\n",
    "                fn = os.path.basename(filename)\n",
    "                if 'sarcasm+yeahright+justkidding-5thMarch-' not in fn:\n",
    "                    temp['#type'] = '#'+remove_trailing_nums(os.path.basename(filename)[:-4])\n",
    "                tweets.append(temp)\n",
    "                    \n",
    "for filename in glob.iglob('data/scraped/*.pkl'):\n",
    "    contents[filename] = pickle.load(open(filename,'rb'))\n",
    "    for d in contents[filename]:\n",
    "        if d['id'] not in alreadyseen:\n",
    "            alreadyseen.add(d['id'])\n",
    "            if type(d['text']) == bytes:\n",
    "                d['text'] = str(d['text'],'utf-8')\n",
    "            temp = {'id':d['id'],'label':'SARCASM','text':d['text']}\n",
    "            fn = os.path.basename(filename)\n",
    "            if fn == 'scraped-with-JUSTKIDDING-until-30Jun2015.pkl':\n",
    "                temp['#type'] = '#justkidding'\n",
    "            elif fn == 'scraped-with-NOT-until-28Dec2015.pkl':\n",
    "                temp['#type'] = '#not'\n",
    "            tweets.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('/home/nikhil/Desktop/245/Project/code/ark-tweet-nlp-0.3.2/scraped-orig.json','w') as fw:\n",
    "    for d in tweets:\n",
    "        fw.write(json.dumps(d)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#with open('/home/nikhil/Desktop/245/Project/code/ark-tweet-nlp-0.3.2/scraped-orig-noretweets.json','w') as fw:\n",
    "    for d in tweets:\n",
    "        if d['text'][:3] != 'RT '\n",
    "        fw.write(json.dumps(d)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets.append({'appeasing_dummy':None})\n",
    "            \n",
    "i = 0\n",
    "tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "with open('/home/nikhil/Desktop/245/Project/code/ark-tweet-nlp-0.3.2/scraped-orig.tagged') as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '':\n",
    "            i += 1\n",
    "            tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "        else:\n",
    "            word,tag,conf = line.strip().split()\n",
    "            tweets[i]['tokens'].append(word)\n",
    "            tweets[i]['tags'].append(tag)\n",
    "            tweets[i]['conf'].append(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(tweets,open('march27/scraped-sarc-tokenized-and-tagged.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets_nort = [tweet for tweet in tweets if tweet['text'][:3]!='RT ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65486"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_nort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets = tweets_nort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets.append({'appeasing_dummy':None})\n",
    "            \n",
    "i = 0\n",
    "tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "with open('/home/nikhil/Desktop/245/Project/code/ark-tweet-nlp-0.3.2/scraped-orig-noretweets.tagged') as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '':\n",
    "            i += 1\n",
    "            tweets[i].update({'tokens':[],'tags':[],'conf':[]})\n",
    "        else:\n",
    "            word,tag,conf = line.strip().split()\n",
    "            tweets[i]['tokens'].append(word)\n",
    "            tweets[i]['tags'].append(tag)\n",
    "            tweets[i]['conf'].append(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(tweets,open('march27/scraped-sarc-noretweets-tokenized-and-tagged.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65486"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riloff-tokenized-and-tagged.pkl\r\n",
      "scraped-sarc-noretweets-tokenized-and-tagged.pkl\r\n",
      "scraped-sarc-tokenized-and-tagged.pkl\r\n"
     ]
    }
   ],
   "source": [
    "ls march27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      1.00      0.80         2\n",
      "          1       0.00      0.00      0.00         1\n",
      "          2       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.67      0.60      0.59         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 0]\n",
    "y_pred = [0, 0, 2, 1, 0]\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
